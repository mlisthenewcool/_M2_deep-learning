{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Exam.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"qOMP0WoFBA5f","colab_type":"text"},"cell_type":"markdown","source":["<h1> Examen Deep Learning </h1>\n","\n","Examen de 3 heures, sur machine, tout documents autorisÃ©s. Pour l'anonymisation de votre rendu, veuillez suivre les consignes : \n","\n","- Choisissez un numÃ©ro alÃ©atoire Ã  6 chiffres, le plus alÃ©atoire possible..\n","- Ecrivez ce numÃ©ro sur une feuille, ainsi que votre nom et numÃ©ro d'Ã©tudiant \n","- Pliez cette feuille en 4, et donnez la Ã  la fin de l'examen au moment d'Ã©marger  \n","- Zippez votre fichier notebook, et nommez l'archive avec votre numÃ©ro alÃ©atoire \n","- Envoyez l'archive via la page : http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/upload/upload.php\n","\n","<hr/>\n","\n","Un musÃ©e spÃ©cialisÃ© en peintures reprÃ©sentant des animaux de la savane a malheureusement perdu son fichier d'inventaire qui regroupait des informations prÃ©cieuses sur les 2000 oeuvres du musÃ©e ! Fort heureusement, l'informaticien toujours prÃ©voyant avait conservÃ© une copie des 2000 photos des oeuvres, et il vient juste de suivre une formation de Deep Learning ...\n","\n","<center>\n","<img src=\" http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/0.jpg\" width=\"90px\" />\n","<img src=\" http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/1.jpg\" width=\"90px\" />\n","<img src=\" http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/2.jpg\" width=\"90px\" />\n","<img src=\" http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/3.jpg\" width=\"90px\" />\n","<img src=\" http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/4.jpg\" width=\"90px\" />\n","<img src=\" http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/5.jpg\" width=\"90px\" />\n","<img src=\" http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/6.jpg\" width=\"90px\" />\n","<img src=\" http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/7.jpg\" width=\"90px\" />\n","<img src=\" http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/8.jpg\" width=\"90px\" />\n","<img src=\" http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/9.jpg\" width=\"90px\" />\n","</center>\n","\n","Cet examen porte sur la rÃ©solution d'une tÃ¢che de classification d'images sur un jeu de donnÃ©es faiblement annotÃ©. Plusieurs solutions sont envisagÃ©es  consistant Ã  tirer parti d'annotations de classes Ã©quivalentes sur un autre dataset entiÃ¨rement Ã©tiquettÃ© (imagenet). Les deux jeux de donnÃ©es contiennent 2000 images rÃ©parties (Ã©galement) en quatre classes (d'animaux : zÃ¨bres, gorilles, lÃ©opards, tigres). 1500 images sont utilisÃ©es pour l'entrainement, 500 pour l'Ã©valuation. Seul le jeu de donnÃ©es A (imagenet) est complÃ¨tement annotÃ©, le dataset B (les photos des oeuvres du musÃ©e) ne contient que trÃ¨s peu d'annotations.\n","\n","La partie 1 consiste Ã  entrainer et Ã©valuer les performances d'un modÃ¨le CNN fourni sur les deux datasets d'images. Les parties 2 et 3 consistent Ã  mettre en oeuvre deux solutions par <i>adaptation de domaines</i> pour tenter d'amÃ©liorer les performances de classification sur le dataset faiblement annotÃ©. Vous pouvez consacrer environ une heure Ã  chaque partie, notÃ©es Ã©galement.\n"]},{"metadata":{"id":"JhEFJi2Jm-AU","colab_type":"text"},"cell_type":"markdown","source":["<h3> TÃ©lÃ©chargement des donnÃ©es </h3>"]},{"metadata":{"id":"xOoNnEmRXA0O","colab_type":"code","colab":{}},"cell_type":"code","source":["!wget http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/imagenetXtrain.npy\n","!wget http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/imagenetYtrain.npy\n","!wget http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/imagenetXtest.npy\n","!wget http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/imagenetYtest.npy\n","  \n","!wget http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/awaXtrain.npy\n","!wget http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/awaYtrain.npy\n","!wget http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/awaXtest.npy\n","!wget http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/awaYtest.npy  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"JELpwwuImeSc","colab_type":"text"},"cell_type":"markdown","source":["<h3>Chargement des donnÃ©es</h3>"]},{"metadata":{"id":"fbvBt_nnAkmy","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","from random import shuffle\n","import keras\n","import keras.backend as K\n","from keras.layers import Input, Dense, Flatten, LeakyReLU\n","from keras.layers import AveragePooling2D, Conv2D, Activation, BatchNormalization, Dropout\n","from keras.models import Model\n","from keras.optimizers import Adam"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hHDrGe_IYIUm","colab_type":"code","colab":{}},"cell_type":"code","source":["path = './'\n","num_classes = 4\n","\n","x_trainA = np.load(path+'imagenetXtrain.npy')\n","x_trainA = x_trainA / 255.\n","y_trainA = np.load(path+'imagenetYtrain.npy')\n","y_trainA = keras.utils.to_categorical(y_trainA, num_classes)\n","\n","x_testA = np.load(path+'imagenetXtest.npy')\n","x_testA = x_testA / 255.\n","y_testA = np.load(path+'imagenetYtest.npy')\n","y_testA = keras.utils.to_categorical(y_testA, num_classes)\n","\n","x_trainB = np.load(path+'awaXtrain.npy')\n","x_trainB = x_trainB / 255.\n","y_trainB = np.load(path+'awaYtrain.npy')\n","y_trainB = keras.utils.to_categorical(y_trainB, num_classes)\n","\n","x_testB = np.load(path+'awaXtest.npy')\n","x_testB = x_testB / 255.\n","y_testB = np.load(path+'awaYtest.npy')\n","y_testB = keras.utils.to_categorical(y_testB, num_classes)\n","\n","print('x_trainA.shape:', x_trainA.shape, 'y_trainA.shape:', y_trainA.shape)\n","print('x_trainB.shape:', x_trainB.shape, 'y_trainB.shape:', y_trainB.shape)\n","\n","print('x_testA.shape:', x_testA.shape, 'y_testA.shape:', y_testA.shape)\n","print('x_testB.shape:', x_testB.shape, 'y_testB.shape:', y_testB.shape)\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nwLsluNtYHkG","colab_type":"text"},"cell_type":"markdown","source":["Les deux jeux de donnÃ©es contiennent 2000 images, de 64x64x3 pixels, rÃ©parties en quatre classes. 1500 images sont utilisÃ©es pour l'entrainement, 500 pour l'Ã©valuation. Seul le jeu de donnÃ©es A est complÃ¨tement annotÃ©, le jeu de donnÃ©e B ne contient que trÃ¨s peu d'annotations, correspondantent aux 50 premiers exemples de x_trainB."]},{"metadata":{"id":"-NRp44rQagA-","colab_type":"text"},"cell_type":"markdown","source":["<h2> Partie 1 : ModÃ¨le convolutionnel pour la classification d'images </h2>\n","\n","On dÃ©finie ci-dessous une architecture convolutionnelle simple, Ã  ne pas modifier. Cette partie vise Ã  Ã©valuer la performance d'un modÃ¨le CNN sur les deux dataset A et B. Entrainez ce modÃ¨le des diffÃ©rentes maniÃ¨res possibles et reportez les performances obtenues dans le tableau situÃ© en fin de cette partie. A chaque nouvel apprentissage, utilisez les configurations suivantes : \n","- 30 epochs\n","- Taille des minibatchs Ã  64\n","- 10% des donnÃ©es pour validation\n","\n","Tracez les courbes d'apprentissage, et commentez vos rÃ©sultats."]},{"metadata":{"id":"aVzDIs5-a_jI","colab_type":"code","colab":{}},"cell_type":"code","source":["# Couches d'apprentissage de reprÃ©sentations par convolutions \n","ximg = Input(shape=(x_trainA.shape[1], x_trainA.shape[2], x_trainA.shape[3]))\n","\n","l = Conv2D(64, kernel_size=(3,3), strides=(1,1), padding='same')(ximg)\n","l = BatchNormalization(axis=-1)(l)\n","l = Activation('relu')(l)\n","l = Conv2D(128, kernel_size=(3,3), strides=(1,1), padding='same')(l)\n","l = BatchNormalization(axis=-1)(l)\n","l = Activation('relu')(l)\n","l = Conv2D(128, kernel_size=(3,3), strides=(1,1), padding='same')(l)\n","l = BatchNormalization(axis=-1)(l)\n","l = Activation('relu')(l)\n","l = AveragePooling2D((4,4))(l)\n","l = Conv2D(256, kernel_size=(3,3), strides=(2,2), padding='same')(l)\n","l = BatchNormalization(axis=-1)(l)\n","l = Conv2D(256, kernel_size=(3,3), strides=(1,1), padding='same')(l)\n","l = AveragePooling2D((2,2))(l)\n","l = Activation('relu')(l)\n","l = Dropout(0.2)(l)\n","feat_layer = Flatten()(l)\n","\n","# conv_model(ximg) retourne la couche 'feat_layer' : 4096 dimensions pour une image 64x64x3\n","conv_model = Model(ximg, feat_layer) \n","\n","# Couches de classification\n","xfeat = Input(shape=(4096,))\n","y = Dense(1024, activation='relu')(xfeat)\n","y = Dropout(0.5)(y)\n","y = Dense(512, activation='relu')(y)\n","out_layer = Dense(num_classes, activation='softmax')(y)\n","\n","# classif_model(xfeat) renvoie la couche sortie du classifieur : 4 dimensions\n","classif_model = Model(xfeat, out_layer) \n","\n","# ModÃ¨le CNN : reprÃ©sentation + classifier\n","feat = conv_model(ximg)\n","clf = classif_model(feat)\n","model = Model(ximg, clf)\n","model.compile(loss='categorical_crossentropy', optimizer=Adam(0.0001), metrics=['accuracy'])\n","print(model.summary())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0-0G5SHsaX06","colab_type":"text"},"cell_type":"markdown","source":["<h4> Entrainement du modÃ¨le sur les donnÃ©es A, puis Ã©valuation sur A et B</h4>"]},{"metadata":{"id":"8GvEjDRqbNXm","colab_type":"code","colab":{}},"cell_type":"code","source":["# A complÃ©ter "],"execution_count":0,"outputs":[]},{"metadata":{"id":"eZgXh4ZmOqkG","colab_type":"text"},"cell_type":"markdown","source":["Commentaires :"]},{"metadata":{"id":"aDIVGF9dbnSO","colab_type":"text"},"cell_type":"markdown","source":["<h4> Entrainement et Ã©valuation du modÃ¨le sur les donnÃ©es B</h4>\n","\n","<i>Attention Ã  ne pas rÃ©utiliser la variable <b>model</b>, dÃ©jÃ  entrainÃ©e sur A, il faut crÃ©er une nouvelle instance <b>model2</b> Ã  partir des fonctions <b>conv_model()</b> et <b>classif_model()</b> dÃ©finient prÃ©cÃ©demment.</i>"]},{"metadata":{"id":"uVnJo3ZUbtRD","colab_type":"code","colab":{}},"cell_type":"code","source":["# A complÃ©ter "],"execution_count":0,"outputs":[]},{"metadata":{"id":"IQ82M78uxDXD","colab_type":"text"},"cell_type":"markdown","source":["Commentaires :"]},{"metadata":{"id":"083Zv7bAb6eR","colab_type":"text"},"cell_type":"markdown","source":["<h4>Finetuning sur B du modÃ¨le entrainÃ© sur A</h4>"]},{"metadata":{"id":"xXQU24qAcBcy","colab_type":"code","colab":{}},"cell_type":"code","source":["# A complÃ©ter "],"execution_count":0,"outputs":[]},{"metadata":{"id":"G-3j2kTBxEat","colab_type":"text"},"cell_type":"markdown","source":["Commentaires :"]},{"metadata":{"id":"rGTo8Gnyx-8s","colab_type":"text"},"cell_type":"markdown","source":["<table>\n","  <thead>\n","    <th></th><th>Accuracy train A</th><th>Accuracy dev A</th><th>Accuracy test A</th><th>Accuracy train B</th><th>Accuracy dev B</th><th>Accuracy test B</th>\n","  </thead>\n","  <tbody>\n","    <tr><th>Entrainement sur A</th><td>###</td><td>###</td><td>###</td><td>###</td><td>###</td><td>###</td></tr>\n","    <tr><th>Entrainement sur B</th><td>###</td><td>###</td><td>###</td><td>###</td><td>###</td><td>###</td></tr>\n","    <tr><th>Entrainement sur A <br/>+ Finetuning sur B</th><td>###</td><td>###</td><td>###</td><td>###</td><td>###</td><td>###</td></tr>\n","  </tbody>\n","</table>\n","\n","Commentaires :\n","\n"]},{"metadata":{"id":"8vv5WxG2cRgu","colab_type":"text"},"cell_type":"markdown","source":["\n","<h2>Partie 2 : Adaptation de domaine par alignement des activations</h2>\n","\n","Cette partie consiste Ã  implÃ©menter et Ã©valuer un modÃ¨le inspirÃ© de l'article <a href=\"https://arxiv.org/pdf/1607.01719.pdf\">Deep CORAL: Correlation Alignment for Deep Domain Adaptation [Sun & Saenko, 2016] </a>, dont la figure suivante illustre le fonctionnement. \n","\n","<center><img src=\"http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/coral16.png\" width=\"50%\" /></center>\n","\n","ConsidÃ©rez les instructions suivantes :\n","- Utilisez les mÃªmes modules de convolutions que dans la Partie 1 afin de rester comparable. Pour cela, considÃ©rez une nouvelle instance <b>conv_model3</b> comme effectuÃ© prÃ©cÃ©demment.\n","- Seules les couches de convolutions sont partagÃ©es.\n","- Seulement deux couches denses aprÃ¨s les convolutions (ie: fc6 et fc7), dropout entre les deux.\n","- Version simplifiÃ©e de la <i>CORAL loss</i> : minimisation de la distance entre les derniÃ¨res couches denses (et/ou maximisation de la corrÃ©lation). Utilisez l'une des couches documentÃ©es sur la page suivante : https://keras.io/layers/merge/ \n","- Entrainez pendant 50 epochs, avec minibatchs = 128\n"]},{"metadata":{"id":"vhO-2mGcErz7","colab_type":"text"},"cell_type":"markdown","source":["<h4> DÃ©finition du modÃ¨le </h4>\n","\n","xA et xB sont les donnÃ©es des deux domaines, yA est la sortie du classifieur pour les donnÃ©es de xA dans le domaine source, et <i>coral</i> correspond Ã  la mÃ©trique (distance ou corrÃ©lation) entre un minibatch d'exemples xA et xB, dans les deux domaines. Le modÃ¨le optimise les deux loss : <i>categorical_crossentropy</i> pour classer les images du domaine source ; et <i>mse</i> qui minimise (ou maximise) la mÃ©trique considÃ©rÃ©e.\n"]},{"metadata":{"id":"gCf5TACncWUd","colab_type":"code","colab":{}},"cell_type":"code","source":["xA = Input(shape=(x_trainA.shape[1], x_trainA.shape[2], x_trainA.shape[3]))\n","xB = Input(shape=(x_trainA.shape[1], x_trainA.shape[2], x_trainA.shape[3]))\n","\n","# A complÃ©ter\n","\n","model4 = Model([xA,xB],[yA, coral])\n","model4.compile(loss=['categorical_crossentropy','mse'], optimizer=Adam(0.0001), metrics=['accuracy'])\n","print(model4.summary())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RKBerCO6mKyQ","colab_type":"text"},"cell_type":"markdown","source":["<h4> Entrainement du modÃ¨le d'adaptation par alignement </h4>\n","\n","Selon le critÃ¨re <i>coral</i> que vous optimisez, considÃ©rez le vecteur objectif ne contenant que des 1 ou que des 0.."]},{"metadata":{"id":"0R1UYCtvmRK7","colab_type":"code","colab":{}},"cell_type":"code","source":["ones = np.ones((len(x_trainA),1)) # ou zeros = np.zeros((len(x_trainA),1))\n","\n","# A complÃ©ter"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VUk9ZasIICOJ","colab_type":"text"},"cell_type":"markdown","source":["Commentaires :"]},{"metadata":{"id":"sP_v8XaKcZJd","colab_type":"text"},"cell_type":"markdown","source":["<h4>Evaluation sur les datasets A et B</h4>"]},{"metadata":{"id":"cTo7eJ6icecX","colab_type":"code","colab":{}},"cell_type":"code","source":["# A complÃ©ter"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wOdK5HRQcf-0","colab_type":"text"},"cell_type":"markdown","source":["Commentaires :"]},{"metadata":{"id":"kP6p06aYm6LB","colab_type":"text"},"cell_type":"markdown","source":["<h2>Partie 3 : Adaptation de domaine par contrainte adversarial</h2>\n","\n","Cette partie consiste Ã  implÃ©menter un modÃ¨le d'adaptation de modÃ¨le par contrainte adversarial, inspirÃ© de l'article <a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper.pdf\">Adversarial Discriminative Domain Adaptation [Tzeng, 2017]</a>.\n","\n","<center><img src=\"http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/adversadapt17.png\" width=\"80%\" /></center>\n","\n","Dans la figure ci-dessus, les modules en pointillÃ©s indiquent une rÃ©utilisation de modÃ¨les dÃ©jÃ  entrainÃ©s, tandis que les modules en lignes pleines dÃ©signent des modÃ¨les nouvellement entrainÃ©s. Pour la phase de </i>Pre-training</i>, considÃ©rez le modÃ¨le entrainÃ© sur le dataset A dans la partie 1. \n","\n","La stratÃ©gie consiste donc Ã  :\n","- Faire le prÃ©training du modÃ¨le source sur les donnÃ©es source avec un critÃ¨re de classification. Cette Ã©tape a Ã©tÃ© faite en partie 1 : <b>conv_model</b> et <b>classif_model</b> sont dÃ©jÃ  entrainÃ©s.\n","- Ne pas rÃ©entrainer ces deux modules.\n","- Apprendre from scratch le modÃ¨le extracteur de caractÃ©ristiques pour les donnÃ©es cible Ã  l'aide d'un discriminateur adversarial appris Ã  discriminer entre les donnÃ©es source transformÃ©es par le modÃ¨le prÃ©appris et les donnÃ©es du domaine cible transformÃ©es par le modÃ¨le cible.  \n","- Construire un classifieur en empilant la couche de classification du modÃ¨le source sur l'extracteur de caractÃ©ristiques sur les donnÃ©es cible.\n","\n"]},{"metadata":{"id":"nGHcH8dz_RxC","colab_type":"text"},"cell_type":"markdown","source":["<h4> DÃ©finition du modÃ¨le discriminateur </h4>\n","Le discriminateur Ã  pour objectif de distinguer les domaines de deux sources de donnÃ©es (A et B) Ã  parir de reprÃ©sentations extraites de couches de convolutions. Ecrivez un modÃ¨le simple Ã  deux couches cachÃ©es. "]},{"metadata":{"id":"miTfCEHBJLSh","colab_type":"code","colab":{}},"cell_type":"code","source":["# A complÃ©ter"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HWrauoL8ZJ1R","colab_type":"text"},"cell_type":"markdown","source":["<h4> ModÃ¨le adversarial </h4>\n","\n","Ecrivez ci-dessous le modÃ¨le adversarial qui combine le module <i>Target CNN</i> et le discriminateur. <i>Target CNN</i> consistera en une nouvelle instance <b>conv_model4</b> pour extraire des reprÃ©sentations similaires au modÃ¨le entrainÃ© en Partie 1 sur les images sources."]},{"metadata":{"id":"INLzkXpmbEnf","colab_type":"code","colab":{}},"cell_type":"code","source":["# A complÃ©ter"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NhKa_hKDcp9m","colab_type":"text"},"cell_type":"markdown","source":["<h4> ModÃ¨le utilisÃ© en phase de test </h4>\n","\n","Construisez ci-dessous le modÃ¨le final qui combine les modules <i>Target CNN</i>  (<b>conv_model4</b>) entrainÃ©s prÃ©cÃ©demment et <b>classif_model</b> entrainÃ© en Partie 1."]},{"metadata":{"id":"Po8eUTuZc_JT","colab_type":"code","colab":{}},"cell_type":"code","source":["# A complÃ©ter"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DZtiQNMiduFd","colab_type":"text"},"cell_type":"markdown","source":["<h4> Entrainement du modÃ¨le adversarial</h4>\n","\n","Utilisez les fonctions de crÃ©ation de minibatchs suivantes qui randomisent les ensembles d'apprentissage Ã  chaque nouvel epoch.\n"]},{"metadata":{"id":"BiWTQLVpeVxc","colab_type":"code","colab":{}},"cell_type":"code","source":["batch_size = 64\n","nbdata = len(x_trainA)\n","\n","def get_batchA():\n","    global x_trainA, y_trainA\n","    i = 0\n","    while True:\n","        i = i + batch_size\n","        if i+batch_size > nbdata:\n","            i = 0\n","            lidx = list(range(nbdata))\n","            shuffle(lidx)\n","            x_trainA = x_trainA[lidx]\n","            y_trainA = y_trainA[lidx]\n","        yield x_trainA[i:i+batch_size], y_trainA[i:i+batch_size]\n","\n","def get_batchB():\n","    global x_trainB, y_trainB\n","    i = 0\n","    while True:\n","        i = i + batch_size\n","        if i+batch_size > nbdata:\n","            i = 0\n","            lidx = list(range(nbdata))\n","            shuffle(lidx)\n","            x_trainB = x_trainB[lidx]\n","        yield x_trainB[i:i+batch_size]\n","\n","data_genA = get_batchA()\n","data_genB = get_batchB()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dKV6F6K9e1QD","colab_type":"text"},"cell_type":"markdown","source":["<h4> Boucle d'apprentissage </h4>"]},{"metadata":{"id":"J3Nw9iZce6Ly","colab_type":"code","colab":{}},"cell_type":"code","source":["ones = np.ones((batch_size,1))\n","zeros = np.zeros((batch_size,1))\n","nb_batchs = int(len(x_trainA)/batch_size)\n","\n","for epoch in range(30):\n","    loss = 0.0\n","    acc = 0.0\n","    for batch in range(nb_batchs):\n","        # get minibatchs\n","        xA, yA = next(data_genA)\n","        xB = next(data_genB)\n","\n","        # A complÃ©ter\n","        \n","        # train discriminator\n","        \n","        # train adversarial model on new minibatch\n","        \n","        # monitoring\n","        \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ruaBJXzFD608","colab_type":"text"},"cell_type":"markdown","source":["Commentaires :"]},{"metadata":{"id":"t8gWmvhPD_p8","colab_type":"text"},"cell_type":"markdown","source":["<h4> Evaluation sur les dataset A et B </h4>"]},{"metadata":{"id":"tNSAXf6cD1FU","colab_type":"code","colab":{}},"cell_type":"code","source":["# A complÃ©ter"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2NT_OtnfEWlm","colab_type":"text"},"cell_type":"markdown","source":["Commentaires :"]},{"metadata":{"id":"OSvJq4lFRG0b","colab_type":"text"},"cell_type":"markdown","source":["<hr/>\n","\n","<b>Rappel, pour l'anonymisation de votre rendu, veuillez suivre les consignes suivantes : </b>\n","\n","- Choisissez un numÃ©ro alÃ©atoire Ã  6 chiffres, le plus alÃ©atoire possible..\n","- Ecrivez ce numÃ©ro sur une feuille, ainsi que votre nom et numÃ©ro d'Ã©tudiant \n","- Pliez cette feuille en 4, et donnez la Ã  la fin de l'examen au moment d'Ã©marger  \n","- Zippez votre fichier notebook, et nommez l'archive avec votre numÃ©ro alÃ©atoire \n","- Envoyez l'archive via la page : http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/upload/upload.php\n"]}]}
